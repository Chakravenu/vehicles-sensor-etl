{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Cleans and preprocesses the data.\n",
    "    \"\"\"\n",
    "    # Handle missing values\n",
    "    df['engine_temperature'] = df['engine_temperature'].fillna(df['engine_temperature'].mean())\n",
    "    df['tire_pressure'] = df['tire_pressure'].fillna(df['tire_pressure'].mean())\n",
    "    df['engine_rpm'] = df['engine_rpm'].fillna(df['engine_rpm'].median())\n",
    "    df['vehicle_speed'] = df['vehicle_speed'].fillna(df['vehicle_speed'].mean())\n",
    "\n",
    "    # Normalize numerical features\n",
    "    cols_to_normalize = ['engine_temperature', 'tire_pressure', 'engine_rpm', 'vehicle_speed', 'mileage']\n",
    "    scaler = MinMaxScaler()\n",
    "    df[cols_to_normalize] = scaler.fit_transform(df[cols_to_normalize])\n",
    "\n",
    "    # Feature engineering\n",
    "    # This is not required as it is redundant data\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "    df['is_weekend'] = df['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "    df['engine_temp_speed'] = df['engine_temperature'] * df['vehicle_speed']\n",
    "    print(\"Data transformation successful!\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_maintenance_analysis(data):\n",
    "    \"\"\"\n",
    "    Generates maintenance analysis by aggregating features based on the maintenance_required column.\n",
    "    \n",
    "    Parameters:\n",
    "    data (DataFrame): Input dataset containing features and the maintenance_required column.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: Aggregated maintenance analysis dataset.\n",
    "    \"\"\"\n",
    "    # Aggregated statistics by maintenance_required\n",
    "    analysis = data.groupby('maintenance_required').agg(\n",
    "        count=('maintenance_required', 'size'),\n",
    "        average_mileage=('mileage', 'mean'),\n",
    "        average_engine_temperature=('engine_temperature', 'mean'),\n",
    "        average_tire_pressure=('tire_pressure', 'mean')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Calculate proportions\n",
    "    total_vehicles = analysis['count'].sum()\n",
    "    analysis['proportion (%)'] = (analysis['count'] / total_vehicles) * 100\n",
    "\n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_models(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Train and evaluate Logistic Regression, Random Forest, and XGBoost.\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(random_state=42),\n",
    "        \"Random Forest\": RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "        \"XGBoost\": XGBClassifier(random_state=42, n_estimators=100, use_label_encoder=False)\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"F1 Score\": f1,\n",
    "            \"ROC-AUC\": roc_auc\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def plot_results(results):\n",
    "    \"\"\"\n",
    "    Plot the performance metrics of the models.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "    # Accuracy Plot\n",
    "    ax[0].bar(results['Model'], results['Accuracy'])\n",
    "    ax[0].set_title(\"Accuracy Comparison\")\n",
    "    ax[0].set_ylabel(\"Accuracy\")\n",
    "\n",
    "    # F1 Score Plot\n",
    "    ax[1].bar(results['Model'], results['F1 Score'])\n",
    "    ax[1].set_title(\"F1 Score Comparison\")\n",
    "    ax[1].set_ylabel(\"F1 Score\")\n",
    "\n",
    "    # ROC-AUC Plot\n",
    "    ax[2].bar(results['Model'], results['ROC-AUC'])\n",
    "    ax[2].set_title(\"ROC-AUC Comparison\")\n",
    "    ax[2].set_ylabel(\"ROC-AUC\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost_model(X_test, X_train,y_test, y_train):\n",
    "    # Train XGBoost model\n",
    "    xgb_model = XGBClassifier(random_state=42)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plot_importance(xgb_model, importance_type='weight')  # Use 'gain' or 'cover' if preferred\n",
    "    plt.title('XGBoost Feature Importance')\n",
    "    plt.show()\n",
    "    \n",
    "     # Make predictions\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "    y_pred_prob = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Combine data for saving\n",
    "    results_df = X_test.copy()\n",
    "    results_df['actual'] = y_test.values\n",
    "    results_df['predicted'] = y_pred\n",
    "    results_df['predicted_probability'] = y_pred_prob\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
